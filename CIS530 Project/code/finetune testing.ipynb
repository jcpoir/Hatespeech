{"cells":[{"cell_type":"markdown","metadata":{"id":"S2nNUNSR5qa5"},"source":["# Setup\n","Click the file icon on the left, then the upload file icon in the panel. Upload `implicit_hate_train.csv`, `implicit_hate_dev.csv`, and `implicit_hate_test.csv`"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"MryTfUvuG5t6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671743830006,"user_tz":300,"elapsed":15466,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}},"outputId":"f5075719-6107-4f02-df83-f5b9a98b3640"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"lGVBFfdLHh_-","executionInfo":{"status":"ok","timestamp":1671743830209,"user_tz":300,"elapsed":205,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","source":["from imblearn.under_sampling import RandomUnderSampler"],"metadata":{"id":"cvL75cbo56OR","executionInfo":{"status":"ok","timestamp":1671743831621,"user_tz":300,"elapsed":1414,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"xogH8S4bH49j","executionInfo":{"status":"ok","timestamp":1671743831621,"user_tz":300,"elapsed":5,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["folder = '/content/drive/MyDrive/CIS 530 project/'"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"De24gtyYyOUh","executionInfo":{"status":"ok","timestamp":1671743831622,"user_tz":300,"elapsed":5,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["import sys\n","sys.path.append(folder)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"k2Wb30RK55vw","colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"status":"error","timestamp":1671743831838,"user_tz":300,"elapsed":220,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}},"outputId":"ec212112-ad5f-4994-e067-ebb45d45fa0d"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-cf125f148029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmajor_class_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"implicit_hate_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmajor_class_dev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"implicit_hate_dev.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmajor_class_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"implicit_hate_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/CIS 530 project/implicit_hate_train.csv'"]}],"source":["major_label_2_idx = {'not_hate' : 0, 'implicit_hate': 1, 'explicit_hate' : 2}\n","major_idx_2_label = {0: 'not_hate', 1:'implicit_hate',2:'explicit_hate'}\n","\n","\n","major_class_train_data = pd.read_csv(folder+\"implicit_hate_train.csv\")\n","major_class_dev_data = pd.read_csv(folder+\"implicit_hate_dev.csv\")\n","major_class_test_data = pd.read_csv(folder+\"implicit_hate_test.csv\")\n","\n","for df in [major_class_train_data,major_class_dev_data,major_class_test_data]:\n","  df['labels'] = df['class'].apply(lambda x: major_label_2_idx[x])\n","print(\"major_class relevant keys: `post`, `class`\")\n","print(\"  post: the tweet (str)\")\n","print(\"  class: takes the values `explicit_hate`, `implicit_hate`, or `not_hate` (str)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORakn1f46IfG","executionInfo":{"status":"aborted","timestamp":1671743831838,"user_tz":300,"elapsed":11,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["minor_label_2_idx = {'incitement':0,'inferiority':1,'irony':2,'stereotypical':3,'threatening':4,'white_grievance':5,'other':6}\n","minor_idx_2_label = {0:'incitement', 1:'inferiority',2:'irony', 3:'stereotypical',4:'threatening',5:'white_grievance',6:'other'}\n","\n","\n","minor_class_train_data = major_class_train_data[major_class_train_data['implicit_class'].notnull()].reset_index(drop=True)\n","minor_class_dev_data = major_class_dev_data[major_class_dev_data['implicit_class'].notnull()].reset_index(drop=True)\n","minor_class_test_data = major_class_test_data[major_class_test_data['implicit_class'].notnull()].reset_index(drop=True)\n","\n","for df in [minor_class_train_data,minor_class_dev_data,minor_class_test_data]:\n","  df['labels'] = df['implicit_class'].apply(lambda x: minor_label_2_idx[x])\n","\n","print(\"minor_class relevant keys: `post`, `implicit_class`\")\n","print(\"  post: the tweet (str)\")\n","print(\"  implicit_class: takes the values `incitement`, `inferiority`, `irony`, `stereotypical`, `threatening`, or `white_grievance` (str)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLOU7vtSI2zA","executionInfo":{"status":"aborted","timestamp":1671743831839,"user_tz":300,"elapsed":11,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["major_class_train_data"]},{"cell_type":"code","source":["major_rus = RandomUnderSampler()\n","major_train_balanced, _ = major_rus.fit_resample(major_class_train_data,major_class_train_data['labels'])\n","\n","minor_rus = RandomUnderSampler()\n","minor_train_balanced, _ = minor_rus.fit_resample(minor_class_train_data,minor_class_train_data['labels'])"],"metadata":{"id":"6yAuogfB6Oss","executionInfo":{"status":"aborted","timestamp":1671743831839,"user_tz":300,"elapsed":10,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jK2J2G_T-NQD"},"source":["## Testing BERT finetuning levels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmM1n7nJeOFL","executionInfo":{"status":"aborted","timestamp":1671743831839,"user_tz":300,"elapsed":10,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOsl5jKBZS2f","executionInfo":{"status":"aborted","timestamp":1671743831840,"user_tz":300,"elapsed":11,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["pip install datasets"]},{"cell_type":"code","source":["pip install wandb"],"metadata":{"id":"ckQ15PV7Bl4G","executionInfo":{"status":"aborted","timestamp":1671743831840,"user_tz":300,"elapsed":11,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import wandb"],"metadata":{"id":"2z1PxpwCBnRg","executionInfo":{"status":"aborted","timestamp":1671743831841,"user_tz":300,"elapsed":11,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"id":"Tvk3NkUnExXQ","executionInfo":{"status":"aborted","timestamp":1671743831841,"user_tz":300,"elapsed":11,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.init(project=\"test-project\", entity=\"cis530-project\")"],"metadata":{"id":"5EgfG4o5EOQz","executionInfo":{"status":"aborted","timestamp":1671743831841,"user_tz":300,"elapsed":11,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlqJKJ5JU2WB","executionInfo":{"status":"aborted","timestamp":1671743831842,"user_tz":300,"elapsed":12,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","\n","from torch.utils.data import Dataset, DataLoader\n","import datasets #huggingface\n","from transformers import BertTokenizer, BertModel, BertForSequenceClassification#DistilBertModel, DistilBertTokenizer #change to model type you want, e.g. Bert or AlBert\n","from evaluation import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QC1jSegzkFRd","executionInfo":{"status":"aborted","timestamp":1671743831842,"user_tz":300,"elapsed":12,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jD78tpXDrn-","executionInfo":{"status":"aborted","timestamp":1671743831842,"user_tz":300,"elapsed":11,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["from tqdm.autonotebook import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjeAU8wOqvhF","executionInfo":{"status":"aborted","timestamp":1671743831843,"user_tz":300,"elapsed":12,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URKhuzckLqDi","executionInfo":{"status":"aborted","timestamp":1671743831843,"user_tz":300,"elapsed":12,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["from transformers import Trainer, TrainingArguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_rxsl_-g7h3","executionInfo":{"status":"aborted","timestamp":1671743831844,"user_tz":300,"elapsed":13,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["def tokenize(data):\n","    return tokenizer(data['post'], truncation=True,padding='max_length')\n","def tokenize_dataset(dataset,tokenizer):\n","    return dataset.map(tokenize,batched=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bklfQfN3hjbW","executionInfo":{"status":"aborted","timestamp":1671743831844,"user_tz":300,"elapsed":13,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6Mrrg55u2G5","executionInfo":{"status":"aborted","timestamp":1671743831844,"user_tz":300,"elapsed":13,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["#if needed, change the tokenizer here, this is used in the function to get CLS embeddings\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") #not worried about uppercase vs lowercase\n","#bert = BertModel.from_pretrained(\"bert-base-uncased\") #if needed, change model here\n","#bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gr8xoALo6V-","executionInfo":{"status":"aborted","timestamp":1671743831845,"user_tz":300,"elapsed":13,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["#use pandas instead to map the classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pn_bPYOFur0","executionInfo":{"status":"aborted","timestamp":1671743831845,"user_tz":300,"elapsed":13,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["\"\"\"\n","#used to make datasets and dataloaders.\n","#x = non-tokenized text\n","#y = labels\n","class ImplicitHateDataset(Dataset):\n","  def __init__(self,text,labels):\n","    self.x = text\n","    self.y = labels\n","  def __len__(self):\n","    return len(self.y)\n","  def __getitem__(self,idx):\n","    \n","    return self.x[idx],self.y[idx]\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGdbQ8Y2_yMp","executionInfo":{"status":"aborted","timestamp":1671743831846,"user_tz":300,"elapsed":14,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["class FineTunedBertModel(nn.Module):\n","  def __init__(self,bert_model, output_size):\n","    super(FineTunedBertModel,self).__init__()\n","    self.bert = bert_model\n","    self.dropout = nn.Dropout(p=0.5)\n","    self.feedforward = nn.Linear(768,output_size)\n","  \n","  def forward(self,data):\n","    embeddings = self.bert(**data).last_hidden_state\n","    non_cls_embedding_mean = torch.mean(embeddings[:,1:,:],1)\n","    logits = self.feedforward(self.dropout(non_cls_embedding_mean))\n","    return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hT5gFKxDt6vc","executionInfo":{"status":"aborted","timestamp":1671743831846,"user_tz":300,"elapsed":14,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["class FineTunedClassifier():\n","  def __init__(self,tokenizer,bert_model,label_name,num_unfrozen_layers,epochs,lr,device):\n","    #label_name is class or implicit_class\n","    #train_hyperparams = learning rate, optimizer, ....\n","    \n","    \n","    if label_name == 'class':\n","      self.label_2_idx = {'not_hate' : 0, 'implicit_hate': 1, 'explicit_hate' : 2}\n","      self.idx_2_label = {0: 'not_hate', 1:'implicit_hate',2:'explicit_hate'}\n","      output_size = 3\n","      \n","    elif label_name == 'implicit_class':\n","      self.label_2_idx = {'incitement':0,'inferiority':1,'irony':2,'stereotypical':3,'threatening':4,'white_grievance':5,'other':6}\n","      self.idx_2_label = {0:'incitement', 1:'inferiority',2:'irony', 3:'stereotypical',4:'threatening',5:'white_grievance',6:'other'}\n","      output_size = 6\n","      \n","    self.label2idx_func = np.vectorize(lambda x: self.label_2_idx[x])\n","    self.idx2label_func = np.vectorize(lambda x: self.idx_2_label[x])\n","      \n","    self.tokenizer = tokenizer\n","    \n","    self.label_name = label_name\n","    \n","    self.finetuned_bert = FineTunedBertModel(bert_model,output_size).to(device)\n","\n","    self.epochs = epochs\n","    self.lr = lr\n","    num_bert_layers = len(self.finetuned_bert.bert.encoder.layer)\n","\n","\n","    #make sure num_unfrozen is valid, if not make it 0\n","    #see: https://discuss.huggingface.co/t/how-to-freeze-some-layers-of-bertmodel/917\n","    unfrozen_layers = num_unfrozen_layers\n","    if num_unfrozen_layers not in range(num_bert_layers-1):\n","      print('invalid number of layers specified, will freeze all of Bert')\n","      unfrozen_layers = 0\n","    for param in self.finetuned_bert.bert.encoder.layer[:num_bert_layers - unfrozen_layers-1].parameters():\n","      param.requires_grad = False\n","    \n","\n","    \n","      output_size = 6\n","\n","  def tokenize(self,data):\n","    return self.tokenizer(data['post'], truncation=True,padding='max_length')\n","  def tokenize_dataset(self,dataset):\n","    return dataset.map(self.tokenize,batched=True)\n","\n","  def preprocess(self, data, is_train):\n","    #is_train: boolean whether or not we're working on the training set, to shuffle it.\n","    \n","    hf_dataset = datasets.Dataset.from_pandas(data[['post','labels']])\n","    print('Tokenizing data:')\n","    hf_dataset = self.tokenize_dataset(hf_dataset)\n","    #hf_dataset = hf_dataset.remove_columns(['post'])\n","    hf_dataset.set_format('torch')\n","    #label_name is class or implicit_class\n","    loader = DataLoader(hf_dataset, shuffle=is_train, batch_size=8)\n","    return loader\n","    \n","\n","  \"\"\"\n","  def make_embeddings_array(self,loader):\n","    \n","    tokenizes each post in batches, and gets the embedding for its CLS token \n","    (which is put in while we tokenize)\n","  \n","    Params:\n","    -------\n","    loader: dataloader containing the raw text and labels\n","\n","    Returns:\n","    --------\n","    all_cls_embeddings: a 2D numpy array where each row is the CLS embedding of its corresponding post\n","    \n","    all_cls_embeddings = None\n","    with torch.no_grad():\n","      for texts, labels in tqdm(loader):\n","        tokenized_texts = self.tokenizer(texts,padding=True,truncation=True, return_tensors='pt')\n","        embeddings = self.bert(**tokenized_texts).last_hidden_state\n","        cls_token_embeddings = torch.mean(embeddings[:,1:,:],1)\n","        if all_cls_embeddings is None:\n","          all_cls_embeddings = cls_token_embeddings.numpy()\n","        else:\n","          all_cls_embeddings = np.concatenate((all_cls_embeddings,cls_token_embeddings),axis=0)\n","        \n","    return all_cls_embeddings\n","  \"\"\"\n","\n","\n","  def train(self,data):\n","    self.finetuned_bert.train()\n","    loader = self.preprocess(data,True)\n","    epochs = self.epochs\n","    learning_rate = self.lr\n","    #[p for p in model.parameters() if p.requires_grad()]\n","    optimizer = torch.optim.Adam(self.finetuned_bert.parameters(),lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    wandb.init(config={\n","        \"epochs\":epochs,\n","        \"batch_size\":8,\n","        \"learning_rate\":learning_rate\n","    }\n","    \n","    )\n","\n","    config = wandb.config\n","\n","    for epoch in range(epochs):\n","      print('Training epoch:',epoch+1)\n","      running_loss = 0\n","      #progress_bar = tqdm(range(len(loader)))\n","      for batch in tqdm(loader):\n","        optimizer.zero_grad()\n","        labels = batch['labels'].to(device)\n","        data = {k: torch.Tensor(v).to(device) for k, v in batch.items() if k != 'labels' and k!= 'post'}\n","        #print(data)\n","        \n","        \n","        logits = self.finetuned_bert(data)\n","\n","        loss = criterion(logits,labels)\n","        running_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        wandb.log({\"loss\":loss.item()})\n","        #progress_bar.update(1)\n","      running_loss /=len(loader)\n","      wandb.log({\"running_loss\":running_loss})\n","      print('Loss for epoch',(epoch+1),':',running_loss)\n","\n","\n","  def predict(self,data):\n","    self.finetuned_bert.eval()\n","    loader = self.preprocess(data,False)\n","    all_predictions = None\n","    for batch in loader:\n","      \n","      labels = batch['labels'].to(device)\n","      data = {k: torch.Tensor(v).to(device) for k, v in batch.items() if k != 'labels' and k!= 'post'}\n","      logits = self.finetuned_bert(data)\n","      predicted = torch.argmax(logits,dim=1)\n","      if all_predictions is None:\n","        all_predictions = predicted.detach().cpu().numpy()\n","      else:\n","        all_predictions = np.concatenate((all_predictions,predicted.detach().cpu().numpy()),axis=0)\n","    return self.idx2label_func(all_predictions)\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvSLm0aJFiKK","executionInfo":{"status":"aborted","timestamp":1671743831846,"user_tz":300,"elapsed":14,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["class ClsTokenBert():\n","  def __init__(self,tokenizer,label_name,num_unfrozen_layers,epochs,lr,device):\n","    if label_name == 'class':\n","      self.label_2_idx = {'not_hate' : 0, 'implicit_hate': 1, 'explicit_hate' : 2}\n","      self.idx_2_label = {0: 'not_hate', 1:'implicit_hate',2:'explicit_hate'}\n","      output_size = 3\n","      \n","    elif label_name == 'implicit_class':\n","      self.label_2_idx = {'incitement':0,'inferiority':1,'irony':2,'stereotypical':3,'threatening':4,'white_grievance':5,'other':6}\n","      self.idx_2_label = {0:'incitement', 1:'inferiority',2:'irony', 3:'stereotypical',4:'threatening',5:'white_grievance',6:'other'}\n","      output_size = 6\n","      \n","    self.label2idx_func = np.vectorize(lambda x: self.label_2_idx[x])\n","    self.idx2label_func = np.vectorize(lambda x: self.idx_2_label[x])\n","      \n","    self.tokenizer = tokenizer\n","    \n","    self.label_name = label_name\n","    \n","    self.finetuned_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=output_size).to(device)\n","    \n","\n","    self.epochs = epochs\n","    self.lr = lr\n","    num_bert_layers = len(self.finetuned_bert.bert.encoder.layer)\n","\n","\n","    #make sure num_unfrozen is valid, if not make it 0\n","    #see: https://discuss.huggingface.co/t/how-to-freeze-some-layers-of-bertmodel/917\n","    unfrozen_layers = num_unfrozen_layers\n","    if num_unfrozen_layers not in range(num_bert_layers-1):\n","      print('invalid number of layers specified, will freeze all of Bert')\n","      unfrozen_layers = 0\n","    for param in self.finetuned_bert.bert.encoder.layer[:num_bert_layers - unfrozen_layers-1].parameters():\n","      param.requires_grad = False\n","    \n","\n","    \n","      output_size = 6\n","\n","  def tokenize(self,data):\n","    return self.tokenizer(data['post'], truncation=True,padding='max_length')\n","  def tokenize_dataset(self,dataset):\n","    return dataset.map(self.tokenize,batched=True)\n","\n","  def preprocess(self, data, is_train):\n","    #is_train: boolean whether or not we're working on the training set, to shuffle it.\n","    \n","    hf_dataset = datasets.Dataset.from_pandas(data[['post','labels']])\n","    print('Tokenizing data:')\n","    hf_dataset = self.tokenize_dataset(hf_dataset)\n","    #hf_dataset = hf_dataset.remove_columns(['post'])\n","    hf_dataset.set_format('torch')\n","    #label_name is class or implicit_class\n","    loader = DataLoader(hf_dataset, shuffle=is_train, batch_size=12)\n","    return loader\n","  def train(self,data):\n","    self.finetuned_bert.train()\n","    loader = self.preprocess(data,True)\n","    epochs = self.epochs\n","    learning_rate = self.lr\n","    #[p for p in model.parameters() if p.requires_grad()]\n","    optimizer = torch.optim.Adam(self.finetuned_bert.parameters(),lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    wandb.init(config={\n","        \"epochs\":epochs,\n","        \"batch_size\":8,\n","        \"learning_rate\":learning_rate\n","    }\n","    \n","    )\n","\n","    config = wandb.config\n","\n","    for epoch in range(epochs):\n","      print('Training epoch:',epoch+1)\n","      running_loss = 0\n","      #progress_bar = tqdm(range(len(loader)))\n","      for batch in tqdm(loader):\n","        optimizer.zero_grad()\n","        #print(batch)\n","        labels = batch['labels'].to(device)\n","        data = {k: v for k, v in batch.items() if k != 'labels' and k!= 'post'}\n","        #print(data)\n","        input_ids = data['input_ids'].to(device)\n","        attention_mask = data['attention_mask'].to(device)\n","        token_type_ids = data['token_type_ids'].to(device)\n","\n","        \n","        logits = self.finetuned_bert(input_ids=input_ids,\n","                                     attention_mask=attention_mask,\n","                                     token_type_ids=token_type_ids).logits.to(device)\n","        \n","        loss = criterion(logits,labels)\n","        running_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        wandb.log({\"loss\":loss.item()})\n","        #progress_bar.update(1)\n","      running_loss /=len(loader)\n","      wandb.log({\"running_loss\":running_loss})\n","      print('Loss for epoch',(epoch+1),':',running_loss)\n","    wandb.finish()\n","\n","  def predict(self,data):\n","    self.finetuned_bert.eval()\n","    loader = self.preprocess(data,False)\n","    all_predictions = None\n","    for batch in loader:\n","      \n","      labels = batch['labels'].to(device)\n","      data = {k: v for k, v in batch.items() if k != 'labels' and k!= 'post'}\n","      input_ids = data['input_ids'].to(device)\n","      attention_mask = data['attention_mask'].to(device)\n","      token_type_ids = data['token_type_ids'].to(device)\n","\n","        \n","      logits = self.finetuned_bert(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids).logits\n","      predicted = torch.argmax(logits,dim=1)\n","      if all_predictions is None:\n","        all_predictions = predicted.detach().cpu().numpy()\n","      else:\n","        all_predictions = np.concatenate((all_predictions,predicted.detach().cpu().numpy()),axis=0)\n","    return self.idx2label_func(all_predictions)"]},{"cell_type":"code","source":["class UnfrozenBert():\n","  def __init__(self,tokenizer,label_name,num_unfrozen_layers,epochs,lr,device):\n","    if label_name == 'class':\n","      self.label_2_idx = {'not_hate' : 0, 'implicit_hate': 1, 'explicit_hate' : 2}\n","      self.idx_2_label = {0: 'not_hate', 1:'implicit_hate',2:'explicit_hate'}\n","      output_size = 3\n","      \n","    elif label_name == 'implicit_class':\n","      self.label_2_idx = {'incitement':0,'inferiority':1,'irony':2,'stereotypical':3,'threatening':4,'white_grievance':5,'other':6}\n","      self.idx_2_label = {0:'incitement', 1:'inferiority',2:'irony', 3:'stereotypical',4:'threatening',5:'white_grievance',6:'other'}\n","      output_size = 6\n","      \n","    self.label2idx_func = np.vectorize(lambda x: self.label_2_idx[x])\n","    self.idx2label_func = np.vectorize(lambda x: self.idx_2_label[x])\n","      \n","    self.tokenizer = tokenizer\n","    \n","    self.label_name = label_name\n","    \n","    self.finetuned_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=output_size).to(device)\n","    \n","\n","    self.epochs = epochs\n","    self.lr = lr\n","    num_bert_layers = len(self.finetuned_bert.bert.encoder.layer)\n","\n","    \"\"\"\n","    #make sure num_unfrozen is valid, if not make it 0\n","    #see: https://discuss.huggingface.co/t/how-to-freeze-some-layers-of-bertmodel/917\n","    unfrozen_layers = num_unfrozen_layers\n","    if num_unfrozen_layers not in range(num_bert_layers-1):\n","      print('invalid number of layers specified, will freeze all of Bert')\n","      unfrozen_layers = 0\n","    for param in self.finetuned_bert.bert.encoder.layer[:num_bert_layers - unfrozen_layers-1].parameters():\n","      param.requires_grad = False\n","    \n","\n","    \n","      output_size = 6\n","    \"\"\"\n","  def tokenize(self,data):\n","    return self.tokenizer(data['post'], truncation=True,padding='max_length')\n","  def tokenize_dataset(self,dataset):\n","    return dataset.map(self.tokenize,batched=True)\n","\n","  def preprocess(self, data, is_train):\n","    #is_train: boolean whether or not we're working on the training set, to shuffle it.\n","    \n","    hf_dataset = datasets.Dataset.from_pandas(data[['post','labels']])\n","    print('Tokenizing data:')\n","    hf_dataset = self.tokenize_dataset(hf_dataset)\n","    #hf_dataset = hf_dataset.remove_columns(['post'])\n","    hf_dataset.set_format('torch')\n","    #label_name is class or implicit_class\n","    loader = DataLoader(hf_dataset, shuffle=is_train, batch_size=12)\n","    return loader\n","  def train(self,data):\n","    self.finetuned_bert.train()\n","    loader = self.preprocess(data,True)\n","    epochs = self.epochs\n","    learning_rate = self.lr\n","    #[p for p in model.parameters() if p.requires_grad()]\n","    optimizer = torch.optim.Adam(self.finetuned_bert.parameters(),lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    wandb.init(config={\n","        \"epochs\":epochs,\n","        \"batch_size\":8,\n","        \"learning_rate\":learning_rate\n","    }\n","    \n","    )\n","\n","    config = wandb.config\n","\n","    for epoch in range(epochs):\n","      print('Training epoch:',epoch+1)\n","      running_loss = 0\n","      #progress_bar = tqdm(range(len(loader)))\n","      for batch in tqdm(loader):\n","        optimizer.zero_grad()\n","        #print(batch)\n","        labels = batch['labels'].to(device)\n","        data = {k: v for k, v in batch.items() if k != 'labels' and k!= 'post'}\n","        #print(data)\n","        input_ids = data['input_ids'].to(device)\n","        attention_mask = data['attention_mask'].to(device)\n","        token_type_ids = data['token_type_ids'].to(device)\n","\n","        \n","        logits = self.finetuned_bert(input_ids=input_ids,\n","                                     attention_mask=attention_mask,\n","                                     token_type_ids=token_type_ids).logits.to(device)\n","        \n","        loss = criterion(logits,labels)\n","        running_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        wandb.log({\"loss\":loss.item()})\n","        #progress_bar.update(1)\n","      running_loss /=len(loader)\n","      wandb.log({\"running_loss\":running_loss})\n","      print('Loss for epoch',(epoch+1),':',running_loss)\n","    wandb.finish()\n","\n","  def predict(self,data):\n","    self.finetuned_bert.eval()\n","    loader = self.preprocess(data,False)\n","    all_predictions = None\n","    for batch in loader:\n","      \n","      labels = batch['labels'].to(device)\n","      data = {k: v for k, v in batch.items() if k != 'labels' and k!= 'post'}\n","      input_ids = data['input_ids'].to(device)\n","      attention_mask = data['attention_mask'].to(device)\n","      token_type_ids = data['token_type_ids'].to(device)\n","\n","        \n","      logits = self.finetuned_bert(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids).logits\n","      predicted = torch.argmax(logits,dim=1)\n","      if all_predictions is None:\n","        all_predictions = predicted.detach().cpu().numpy()\n","      else:\n","        all_predictions = np.concatenate((all_predictions,predicted.detach().cpu().numpy()),axis=0)\n","    return self.idx2label_func(all_predictions)"],"metadata":{"id":"FckpldkHsAnR","executionInfo":{"status":"aborted","timestamp":1671743831847,"user_tz":300,"elapsed":18311,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PptB_bo0bw7","executionInfo":{"status":"aborted","timestamp":1671743831847,"user_tz":300,"elapsed":18310,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"markdown","source":["Testing with ALL of Bert unfrozen"],"metadata":{"id":"fzChe2q1KCol"}},{"cell_type":"code","source":["epochs = 10\n","lr = 2e-6\n","num_unfrozen_layers = 12\n","\n","#major_model = FineTunedClassifier(tokenizer,bert,\"class\",256,0,None)\n","major_class_model = UnfrozenBert(tokenizer,\"class\",num_unfrozen_layers,epochs,lr,device)\n","\n","\n","minor_class_model = UnfrozenBert(tokenizer,\"implicit_class\",num_unfrozen_layers,epochs,lr,device)\n"],"metadata":{"id":"2TTwPXStKCN9","executionInfo":{"status":"aborted","timestamp":1671743831848,"user_tz":300,"elapsed":18307,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate(major_class_model, major_train_balanced, major_class_test_data, \"class\")"],"metadata":{"id":"z2julcsoLDYR","executionInfo":{"status":"aborted","timestamp":1671743831848,"user_tz":300,"elapsed":18303,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate(minor_class_model, minor_train_balanced, minor_class_test_data, \"implicit_class\")"],"metadata":{"id":"N5GiFSlKLGMR","executionInfo":{"status":"aborted","timestamp":1671743831848,"user_tz":300,"elapsed":18298,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing with no unfrozen Bert layers"],"metadata":{"id":"DjPrHkIYZ_Ur"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OH_ae-J_Lcd","executionInfo":{"status":"aborted","timestamp":1671743831849,"user_tz":300,"elapsed":18295,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["epochs = 10\n","lr = 2e-5\n","num_unfrozen_layers = 0\n","\n","#major_model = FineTunedClassifier(tokenizer,bert,\"class\",256,0,None)\n","major_class_model = ClsTokenBert(tokenizer,\"class\",num_unfrozen_layers,epochs,lr,device)\n","\n","\n","minor_class_model = ClsTokenBert(tokenizer,\"implicit_class\",num_unfrozen_layers,epochs,lr,device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPZ_agKMFNfC","executionInfo":{"status":"aborted","timestamp":1671743831849,"user_tz":300,"elapsed":18290,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["train_and_evaluate(major_class_model, major_train_balanced, major_class_test_data, \"class\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxPYGigQWpOZ","executionInfo":{"status":"aborted","timestamp":1671743831850,"user_tz":300,"elapsed":18286,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"outputs":[],"source":["train_and_evaluate(minor_class_model, minor_train_balanced, minor_class_test_data, \"implicit_class\")"]},{"cell_type":"markdown","source":["Testing one unfrozen Bert layer"],"metadata":{"id":"vUaDNvDtaDib"}},{"cell_type":"code","source":["num_unfrozen_layers = 1\n","major_class_model = ClsTokenBert(tokenizer,\"class\",num_unfrozen_layers,epochs,lr,device)\n","\n","\n","minor_class_model = ClsTokenBert(tokenizer,\"implicit_class\",num_unfrozen_layers,epochs,lr,device)"],"metadata":{"id":"-DiDbxX2Zxzk","executionInfo":{"status":"aborted","timestamp":1671743831850,"user_tz":300,"elapsed":18282,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate(major_class_model, major_train_balanced, major_class_test_data, \"class\")"],"metadata":{"id":"lNZ_bpKhZ3he","executionInfo":{"status":"aborted","timestamp":1671743831850,"user_tz":300,"elapsed":18278,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate(minor_class_model, minor_train_balanced, minor_class_test_data, \"implicit_class\")"],"metadata":{"id":"HJXInVfnZ5aT","executionInfo":{"status":"aborted","timestamp":1671743831851,"user_tz":300,"elapsed":18274,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing two unfrozen Bert layers"],"metadata":{"id":"1tJHtx4PaFZX"}},{"cell_type":"code","source":["num_unfrozen_layers = 2\n","major_class_model = ClsTokenBert(tokenizer,\"class\",num_unfrozen_layers,epochs,lr,device)\n","\n","\n","minor_class_model = ClsTokenBert(tokenizer,\"implicit_class\",num_unfrozen_layers,epochs,lr,device)"],"metadata":{"id":"2h2kyr37Z6G9","executionInfo":{"status":"aborted","timestamp":1671743831851,"user_tz":300,"elapsed":18270,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate(major_class_model, major_train_balanced, major_class_test_data, \"class\")"],"metadata":{"id":"sWa7gtelZ8Vz","executionInfo":{"status":"aborted","timestamp":1671743831851,"user_tz":300,"elapsed":18266,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate(minor_class_model, minor_train_balanced, minor_class_test_data, \"implicit_class\")"],"metadata":{"id":"bu9quJcaZ9lL","executionInfo":{"status":"aborted","timestamp":1671743831852,"user_tz":300,"elapsed":18262,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing three unfrozen Bert layers"],"metadata":{"id":"qOwlDlLVaJNJ"}},{"cell_type":"code","source":["num_unfrozen_layers = 3\n","major_class_model = ClsTokenBert(tokenizer,\"class\",num_unfrozen_layers,epochs,lr,device)\n","\n","\n","minor_class_model = ClsTokenBert(tokenizer,\"implicit_class\",num_unfrozen_layers,epochs,lr,device)"],"metadata":{"id":"NdyZdUE-aHeC","executionInfo":{"status":"aborted","timestamp":1671743831852,"user_tz":300,"elapsed":18261,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate(major_class_model, major_train_balanced, major_class_test_data, \"class\")"],"metadata":{"id":"K-8LVeKuaMUK","executionInfo":{"status":"aborted","timestamp":1671743831852,"user_tz":300,"elapsed":18256,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate(minor_class_model, minor_train_balanced, minor_class_test_data, \"implicit_class\")"],"metadata":{"id":"n2z_XIKpaNNp","executionInfo":{"status":"aborted","timestamp":1671743831853,"user_tz":300,"elapsed":18253,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A look at the bert encoder layers to see the architecture"],"metadata":{"id":"9lN7YuYsaRRj"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18249,"status":"aborted","timestamp":1671743831853,"user":{"displayName":"Joe Poirier","userId":"17067261562465913669"},"user_tz":300},"id":"Bk7QxxN6Pecx"},"outputs":[],"source":["print(minor_class_model.finetuned_bert.bert.encoder)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1rfynXSIGJBeuDDP1SqNo0C1oRWKKpuLa","timestamp":1670206195345}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}